{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder_LLE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "c5tSJxbb7LIJ"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMPsMclu1cD0",
        "colab_type": "text"
      },
      "source": [
        "### Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NW7PjULhGt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcJDer3WhZVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/M2/DR\"\n",
        "\n",
        "import sys\n",
        "sys.path.append(path)\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDUE3eJpjoEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from math import sqrt\n",
        "\n",
        "dataset = \"MNIST\"\n",
        "dataset = \"USPS\"\n",
        "\n",
        "#################################################\n",
        "# Dataset\n",
        "#################################################\n",
        "\n",
        "# MNIST\n",
        "if dataset == \"MNIST\":\n",
        "    from keras.datasets import mnist\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# USPS\n",
        "if dataset == \"USPS\":\n",
        "    mat = io.loadmat(path+\"/USPS.mat\")\n",
        "    X_train = X_test = mat['X'].reshape((mat['X'].shape[0], int(sqrt(mat['X'].shape[1])), int(sqrt(mat['X'].shape[1]))))\n",
        "    y_train = y_test = mat['y']\n",
        "\n",
        "#####################\n",
        "# Pre processing\n",
        "#####################\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "min_X = min(X_train.min(), X_test.min())\n",
        "max_X = max(X_train.max(), X_test.max())\n",
        "X_train, X_test = (X_train - min_X) / (max_X - min_X), (X_test - min_X) / (max_X - min_X)\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Functions\n",
        "#################################################\n",
        "num_examples_to_generate=16\n",
        "def plot_images(predictions, epoch):\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 255.0, cmap='gray')\n",
        "      #plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  plot_images(predictions, epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liFWbx7orbkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, n_init=20)\n",
        "kmeans.fit(X_train.reshape((60000, 28*28)))\n",
        "\n",
        "print(nmi(kmeans.labels_, y_train))\n",
        "print(ari(kmeans.labels_, y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m913TteujpvI",
        "colab_type": "text"
      },
      "source": [
        "## **1. Deep Autoencoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7cPBvxyy5xW",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuiQnKkSy-fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import Input, layers\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Hyper-parameters\n",
        "#################################################\n",
        "epoch = 100\n",
        "batch_size = 256\n",
        "\n",
        "encoding_dim = 7*7\n",
        "hidden_dim = 512\n",
        "\n",
        "#activation='elu'\n",
        "activation='leaky_relu'\n",
        "optimizer='adam'\n",
        "#loss='mean_squared_error'\n",
        "loss='binary_crossentropy'\n",
        "#################################################\n",
        "\n",
        "out_activation = 'sigmoid' if loss=='binary_crossentropy' else 'linear'\n",
        "activation = None if activation=='leaky_relu' else activation\n",
        "\n",
        "def make_autoencoder_model():\n",
        "    #################################################\n",
        "    # Auto-encoder\n",
        "    #################################################\n",
        "    input_img = Input(shape=(784,))\n",
        "\n",
        "\n",
        "    ####### Encoder #######\n",
        "    #   Layer\n",
        "    encoded = Dense(hidden_dim, activation=activation)(input_img)\n",
        "    if activation is None: encoded = layers.LeakyReLU()(encoded)\n",
        "\n",
        "    #   Layer\n",
        "    encoded = Dense(hidden_dim//4, activation=activation)(encoded)\n",
        "    if activation is None: encoded = layers.LeakyReLU()(encoded)\n",
        "\n",
        "    #   Layer\n",
        "    encoded = Dense(encoding_dim, activation=out_activation)(encoded)\n",
        "\n",
        "\n",
        "    ####### Decoder #######\n",
        "    #   Layer\n",
        "    decoded = Dense(hidden_dim//4, activation=activation)(encoded)\n",
        "    if activation is None: decoded = layers.LeakyReLU()(decoded)\n",
        "\n",
        "    #   Layer\n",
        "    decoded = Dense(hidden_dim, activation=activation)(decoded)\n",
        "    if activation is None: decoded = layers.LeakyReLU()(decoded)\n",
        "\n",
        "    #   Layer\n",
        "    decoded = Dense(784, activation=out_activation)(decoded)\n",
        "\n",
        "\n",
        "    ####### Make model #######\n",
        "    autoencoder = Model(input_img, decoded)\n",
        "\n",
        "    return autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT83MRLly6Ns",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t4xn_WDhH6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "autoencoder = make_autoencoder_model()\n",
        "autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "autoencoder.summary()\n",
        "####### Train #######\n",
        "autoencoder.fit(X_train, X_train,\n",
        "                epochs=epoch,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, X_test))\n",
        "\n",
        "\n",
        "####### Encode images #######\n",
        "encoder = Model(input_img, encoded)\n",
        "encoded_images = encoder.predict(X_train)\n",
        "#################################################\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# KMeans\n",
        "#################################################\n",
        "print(\"run KMeans\")\n",
        "kmeans = KMeans(n_clusters=10, n_init=20)\n",
        "kmeans.fit(encoded_images)\n",
        "\n",
        "print(nmi(kmeans.labels_, y_train))\n",
        "print(ari(kmeans.labels_, y_train))\n",
        "#################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJll9Pazj4dD",
        "colab_type": "text"
      },
      "source": [
        "## **2. Deep Convolutional Autoencoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pjWP6FRuSwF",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5_qJyYio4CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import Input, layers, models\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "from math import sqrt\n",
        "\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Hyper-parameters\n",
        "#################################################\n",
        "epoch = 10\n",
        "batch_size = 256\n",
        "\n",
        "#activation='elu'\n",
        "activation='leaky_relu'\n",
        "optimizer='adam'\n",
        "#loss='mean_squared_error'\n",
        "loss='binary_crossentropy'\n",
        "\n",
        "use_batch_norm_encoded = False\n",
        "use_batch_norm_decoded = False\n",
        "#################################################\n",
        "\n",
        "out_activation = 'sigmoid' if loss=='binary_crossentropy' else 'linear'\n",
        "activation = None if activation=='leaky_relu' else activation\n",
        "\n",
        "def make_conv_autoencoder_model(img_height=28, img_width=28):\n",
        "  big_img = img_height >= 18 and img_width >= 18\n",
        "  divisor = 4 if big_img else 2\n",
        "  encoding_height, encoding_width = img_height//divisor, img_width//divisor\n",
        "  encoding_dim = encoding_height * encoding_width\n",
        "\n",
        "  #################################################\n",
        "  # Convolutional Auto-encoder\n",
        "  #################################################\n",
        "  input_img = Input((img_height, img_width, 1))\n",
        "\n",
        "  ####### Encoder #######\n",
        "  #   Layer\n",
        "  encoded = layers.Conv2D(32, (3, 3), activation=activation)(input_img)\n",
        "  if use_batch_norm_encoded: encoded = layers.BatchNormalization()(encoded)\n",
        "  if activation is None:     encoded = layers.LeakyReLU()(encoded)\n",
        "  encoded = layers.MaxPooling2D((2, 2))(encoded)\n",
        "\n",
        "  #   Layer\n",
        "  encoded = layers.Conv2D(64, (3, 3), activation=activation)(encoded)\n",
        "  if use_batch_norm_encoded: encoded = layers.BatchNormalization()(encoded)\n",
        "  if activation is None:     encoded = layers.LeakyReLU()(encoded)\n",
        "  if big_img:                encoded = layers.MaxPooling2D((2, 2))(encoded) # otherwise we will have dimension issues\n",
        "\n",
        "  #   Layer\n",
        "  encoded = layers.Conv2D(64, (3, 3), activation=activation)(encoded)\n",
        "  if use_batch_norm_encoded: encoded = layers.BatchNormalization()(encoded)                          \n",
        "  if activation is None:     encoded = layers.LeakyReLU()(encoded)\n",
        "\n",
        "  #   Layer\n",
        "  encoded = layers.Flatten()(encoded)\n",
        "  encoded = layers.Dense(encoding_dim, activation=out_activation)(encoded)\n",
        "\n",
        "\n",
        "  ####### Decoder #######\n",
        "  decoded_input = encoded if encoding_dim == encoding_height * encoding_width else layers.Dense(encoding_height * encoding_width)(encoded)\n",
        "  decoded = layers.Reshape((encoding_height, encoding_width, 1))(decoded_input)\n",
        "\n",
        "  #   Layer\n",
        "  strides = 2 if big_img else 1\n",
        "  decoded = layers.Conv2DTranspose(64,(3, 3), strides=strides, activation=activation, padding='same')(decoded)\n",
        "  if use_batch_norm_decoded: decoded = layers.BatchNormalization()(decoded)\n",
        "  if activation is None:     decoded = layers.LeakyReLU()(decoded)\n",
        "\n",
        "  #   Layer\n",
        "  decoded = layers.Conv2DTranspose(64,(3, 3), strides=2, activation=activation, padding='same')(decoded)\n",
        "  if use_batch_norm_decoded: decoded = layers.BatchNormalization()(decoded)\n",
        "  if activation is None:     decoded = layers.LeakyReLU()(decoded)\n",
        "\n",
        "  #   Layer\n",
        "  decoded = layers.Conv2DTranspose(32,(3, 3), activation=activation, padding='same')(decoded)\n",
        "  if use_batch_norm_decoded: decoded = layers.BatchNormalization()(decoded)\n",
        "  if activation is None:     decoded = layers.LeakyReLU()(decoded)\n",
        "\n",
        "  #   Layer\n",
        "  decoded = layers.Conv2D(1, (3, 3), activation=out_activation, padding='same')(decoded)\n",
        "\n",
        "\n",
        "  ####### Make model #######\n",
        "  autoencoder = models.Model(input_img, decoded)\n",
        "  encoder = models.Model(input_img, encoded)\n",
        "  #decoder = models.Model(encoded, decoded)\n",
        "\n",
        "  return autoencoder, encoder, decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7PX9eVuqqI",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqLpAQewhml1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = 100\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "autoencoder, encoder, decoded = make_conv_autoencoder_model(X_train.shape[1], X_train.shape[2])\n",
        "autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "autoencoder.summary()\n",
        "\n",
        "\"\"\"ae_checkpoint_dir = path+'/ae_training_checkpoints'\n",
        "ae_checkpoint_prefix = os.path.join(ae_checkpoint_dir, \"ckpt\")\n",
        "ae_checkpoint = tf.train.Checkpoint(autoencoder=autoencoder)\n",
        "\n",
        "ae_checkpoint.restore(tf.train.latest_checkpoint(ae_checkpoint_dir))\"\"\"\n",
        "\n",
        "\"\"\"for _ in range(8):\n",
        "  generate_and_save_images(autoencoder, 0, \n",
        "                        test_input=X_train[np.random.randint(low=0,high=train_images.shape[0],size=16)])\"\"\"\n",
        "\n",
        "####### Train #######\n",
        "autoencoder.fit(X_train, X_train,\n",
        "                epochs=epoch,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, X_test))\n",
        "\n",
        "#ae_checkpoint.save(file_prefix = ae_checkpoint_prefix)\n",
        "\n",
        "for _ in range(8):\n",
        "  generate_and_save_images(autoencoder, 0, \n",
        "                        test_input=X_train[np.random.randint(low=0,high=X_train.shape[0],size=16)])\n",
        "\n",
        "#################################################\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# KMeans\n",
        "#################################################\n",
        "####### Encode images #######\n",
        "encoded_images = encoder.predict(X_train)\n",
        "print(encoded_images.shape)\n",
        "\n",
        "####### KMeans #######\n",
        "print(\"run KMeans\")\n",
        "kmeans = KMeans(n_clusters=10, n_init=20)\n",
        "kmeans.fit(encoded_images)\n",
        "\n",
        "print(nmi(kmeans.labels_, y_train.ravel()))\n",
        "print(ari(kmeans.labels_, y_train.ravel()))\n",
        "#################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzIB_EMszFFp",
        "colab_type": "text"
      },
      "source": [
        "## **3. LLE**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sju6W-fY7I4B",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIJQbmP94cB",
        "colab_type": "text"
      },
      "source": [
        "\\begin{equation}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_i} = 2G_i w_i - \\lambda \\mathbf{1} \\\\\n",
        "\\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = \\mathbf{1}^T w_i - \\mathbf{1}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXaGt_Hr30bf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from scipy.linalg import solve\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "\n",
        "\n",
        "class LLE():\n",
        "    def __init__(self, n_neighbors=5, n_components=2, n_jobs=None, verbose=True, \n",
        "                 learning_rate=0.0001, neighbors_update=False, method=\"direct\",\n",
        "                 reg=1e-3):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.n_components = n_components\n",
        "        self.n_jobs = n_jobs\n",
        "        self.verbose = verbose        \n",
        "        self.learning_rate = learning_rate        # Used only if method=\"gradient\"\n",
        "        self.neighbors_update = neighbors_update\n",
        "        self.method = method\n",
        "        self.reg = reg\n",
        "\n",
        "        self.W = None                            # Sparse full Weight matrix [n_samples x n_samples]\n",
        "        self.B = None                            # Reduced Weight matrix [n_samples x n_neighbors]\n",
        "        self.lagrange_lambda = 0                 # Lambda (lagrange multiplier)\n",
        "        self.knn = None                          # Store kNN result to avoid computation each time fit is called\n",
        "        self.ind = None                          # Store kNN neighbors indexes\n",
        "\n",
        "    def _update_W(self, regularization=True):\n",
        "        n_samples = self.B.shape[0]\n",
        "        indptr = np.arange(0, n_samples * self.n_neighbors + 1, self.n_neighbors)\n",
        "        rowsum = self.B.sum(1)[:,None]\n",
        "        rowsum[rowsum == 0] = 1 \n",
        "        data = self.B \n",
        "        if regularization:\n",
        "            data = data / rowsum\n",
        "        self.W = sparse.csr_matrix((data.ravel(), self.ind.ravel(), indptr),\n",
        "                      shape=(n_samples, n_samples))\n",
        "\n",
        "    def get_W(self):\n",
        "        self._update_W()\n",
        "        return self.W\n",
        "\n",
        "    def compute_weight_loss(self, X):\n",
        "        W = self.get_W()\n",
        "        X_csr = sparse.csr_matrix(X)\n",
        "        #return np.square(X - np.dot(W.todense(), X)).sum()\n",
        "        return (X_csr - W.dot(X_csr)).power(2).sum()\n",
        "\n",
        "    def _one_fit(self, X):\n",
        "        Z = X[self.ind]\n",
        "        n_samples, n_neighbors = X.shape[0], Z.shape[1]\n",
        "        ones = np.ones(n_neighbors)\n",
        "\n",
        "        for i, A in enumerate(Z.transpose(0, 2, 1)):\n",
        "            #C = A.T - X[i]  \n",
        "            C = X[i] - A.T\n",
        "            G = np.dot(C, C.T)\n",
        "\n",
        "            if self.method == \"gradient\":\n",
        "                # Gradient descent way\n",
        "                w_gradient = 2 * np.dot(G, self.B[i, :]) - self.lagrange_lambda * ones\n",
        "                lambda_gradient = np.dot(ones, self.B[i, :]) - 1\n",
        "\n",
        "                self.lagrange_lambda -= self.learning_rate * lambda_gradient\n",
        "                self.B[i, :] -= self.learning_rate * w_gradient\n",
        "\n",
        "                b_sum = self.B[i, :].sum()\n",
        "                if b_sum != 0: \n",
        "                    self.B[i, :] = self.B[i, :] / b_sum \n",
        "\n",
        "            elif self.method == \"direct\":\n",
        "                # Direct way, from sklearn's \"barycenter_weights\" function\n",
        "                # https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/manifold/_locally_linear.py\n",
        "                trace = np.trace(G)\n",
        "                if trace > 0:\n",
        "                    R = self.reg * trace\n",
        "                else:\n",
        "                    R = self.reg\n",
        "                G.flat[::Z.shape[1] + 1] += R\n",
        "                w = solve(G, ones, sym_pos=True)\n",
        "                self.B[i, :] = w / np.sum(w)\n",
        "\n",
        "\n",
        "    def fit(self, X, epoch=1):\n",
        "        # Run kNN algorithm      \n",
        "        if self.knn is None or self.neighbors_update:\n",
        "            if self.verbose: print(\"Run kNN...\")\n",
        "            self.knn = NearestNeighbors(self.n_neighbors + 1, n_jobs=self.n_jobs).fit(X)\n",
        "            self.ind = self.knn.kneighbors(X, return_distance=False)[:, 1:]\n",
        "        \n",
        "        # Init B matrix\n",
        "        if self.B is None:\n",
        "            \"\"\"self.B = np.random.random((X.shape[0], self.n_neighbors))\n",
        "            self.B /= self.B.sum(1)[:,None]\"\"\"\n",
        "            self.B = np.zeros((X.shape[0], self.n_neighbors))\n",
        "\n",
        "        # Train\n",
        "        if self.verbose: \n",
        "            print(\"Train...\")\n",
        "            #print(\"LLE loss = \", self.compute_weight_loss(X))\n",
        "        for ep in range(epoch):\n",
        "            t0 = time()\n",
        "            self._one_fit(X)\n",
        "            t1 = time()\n",
        "            if self.verbose:\n",
        "                print(\": %.2g sec\" % (t1 - t0))\n",
        "            if self.method != \"direct\": \n",
        "                if ep % 100 == 0 and self.verbose:\n",
        "                    print(ep, \" \\ \", epoch)\n",
        "                    print(\"LLE loss = \", self.compute_weight_loss(X))\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"LLE loss = \", self.compute_weight_loss(X))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return None\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5tSJxbb7LIJ",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8qdLOw27PxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\n",
        "X = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
        "print(X.shape)\n",
        "\n",
        "lle = LLE(method=\"gradient\")\n",
        "lle.fit(X, epoch=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpNfGS_Z3taI",
        "colab_type": "text"
      },
      "source": [
        "## **4. AE LLE**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y66gU3n_I4Eg",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JZMyr1kI7LT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from scipy import sparse\n",
        "\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "def ae_lle_loss(X_encoded, W):\n",
        "    X_encoded_sparse = sparse.csr_matrix(X_encoded)\n",
        "    return mse(X_encoded, W.dot(X_encoded_sparse).todense())\n",
        "\n",
        "def make_ae_lle_optimizer():\n",
        "    return tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pOYva_VI4JO",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOUxqpcG6RA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = 50\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from statistics import mean\n",
        "\n",
        "autoencoder, encoder, decoded = make_conv_autoencoder_model(X_train.shape[1], X_train.shape[2])\n",
        "autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "ae_lle_optimizer = make_ae_lle_optimizer()\n",
        "\n",
        "autoencoder.summary()\n",
        "\n",
        "lle = LLE(neighbors_update=True, verbose=False) # we need to recompute neighbors at each iteration autoencoder encoding changes over time\n",
        "\n",
        "# Train encoder according to the second term of the loss\n",
        "def ae_lle_train_step(X, W):\n",
        "    with tf.GradientTape() as ae_lle_tape:\n",
        "      X_encoded = encoder(X, training=True)\n",
        "      encoder_loss = ae_lle_loss(X_encoded, W)\n",
        "\n",
        "    gradients_of_ae_lle = ae_lle_tape.gradient(encoder_loss, encoder.trainable_variables)\n",
        "\n",
        "    ae_lle_optimizer.apply_gradients(zip(gradients_of_ae_lle, encoder.trainable_variables))\n",
        "\n",
        "    return encoder_loss\n",
        "\n",
        "####### Train #######\n",
        "for ep in range(epoch):\n",
        "    print(\"_________________________________________________________________\")\n",
        "    print(\"epoch : \",ep, \" / \", epoch)\n",
        "    #losses = {\"Autoencoder\":[], \"LLE\":[], \"AE-LLE\":[]}\n",
        "\n",
        "    # Autoencoder Training\n",
        "    #print(\"--- Autoencoder update...\")\n",
        "    autoencoder.fit(X_train, X_train,\n",
        "                    epochs=1,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True, verbose=0,\n",
        "                    validation_data=(X_test, X_test))\n",
        "    results = autoencoder.evaluate(X_train, X_train, batch_size=batch_size, verbose=0)\n",
        "    print(\"Autoencoder loss = \", results)\n",
        "    \n",
        "    # LLE Training\n",
        "    #print(\"--- LLE update...\")\n",
        "    X_encoded = encoder.predict(X_train)\n",
        "    #print(X_encoded.shape)\n",
        "    lle.fit(X_encoded)\n",
        "    print(\"LLE loss         = \", lle.compute_weight_loss(X_encoded))\n",
        "\n",
        "    # Encoder Training\n",
        "    #print(\"--- Encoder update...\")\n",
        "    encoder_losses = []\n",
        "    W = lle.get_W()\n",
        "    \"\"\"for it in range(int(X_train.shape[0] / batch_size)):\n",
        "        indexes = np.random.randint(low=0,high=X_train.shape[0],size=batch_size)\n",
        "        X_batch = X_train[indexes]  \n",
        "        W_batch = W[indexes]\n",
        "        print(W_batch.shape)\n",
        "        print(X_batch.shape)\n",
        "        input()\n",
        "        encoder_loss = ae_lle_train_step(X_batch, W_batch)\n",
        "        encoder_losses.append(encoder_loss)\n",
        "    print(\"Encoder loss = \", mean(encoder_losses))\"\"\"\n",
        "    encoder_loss = ae_lle_train_step(X_train, W)\n",
        "    print(\"Encoder loss     = \", encoder_loss.numpy())\n",
        "\n",
        "for _ in range(8):\n",
        "  generate_and_save_images(autoencoder, 0, \n",
        "                        test_input=X_train[np.random.randint(low=0,high=X_train.shape[0],size=16)])\n",
        "\n",
        "#################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"ae_checkpoint_dir = path+'/ae_training_checkpoints'\n",
        "ae_checkpoint_prefix = os.path.join(ae_checkpoint_dir, \"ckpt\")\n",
        "ae_checkpoint = tf.train.Checkpoint(autoencoder=autoencoder)\n",
        "\n",
        "ae_checkpoint.restore(tf.train.latest_checkpoint(ae_checkpoint_dir))\n",
        "\n",
        "ae_checkpoint.save(file_prefix = ae_checkpoint_prefix)\"\"\"\n",
        "\n",
        "\"\"\"for _ in range(8):\n",
        "  generate_and_save_images(autoencoder, 0, \n",
        "                        test_input=X_train[np.random.randint(low=0,high=train_images.shape[0],size=16)])\"\"\"\n",
        "\n",
        "\"\"\"for it in range(int(X_train.shape[0] / batch_size)):\n",
        "    X_batch = X_train[np.random.randint(low=0,high=X_train.shape[0],size=batch_size)]  \n",
        "    autoencoder.fit(X_batch, X_batch,\n",
        "                    epochs=1,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True, verbose=0)\n",
        "    \n",
        "results = autoencoder.evaluate(X_test, X_test, batch_size=batch_size)\n",
        "print('Autoencoder validation:', results)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}