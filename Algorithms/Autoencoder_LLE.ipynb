{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder_LLE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMPsMclu1cD0",
        "colab_type": "text"
      },
      "source": [
        "### Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NW7PjULhGt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcJDer3WhZVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/M2/DR\"\n",
        "\n",
        "import sys\n",
        "sys.path.append(path)\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDUE3eJpjoEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from math import sqrt\n",
        "\n",
        "datasets = {}\n",
        "\n",
        "#################################################\n",
        "# Dataset\n",
        "#################################################\n",
        "\n",
        "def add_dataset(X_train, y_train, X_test, y_test, dataset_name):\n",
        "    min_X = min(X_train.min(), X_test.min())\n",
        "    max_X = max(X_train.max(), X_test.max())\n",
        "    X_train, X_test = (X_train - min_X) / (max_X - min_X), (X_test - min_X) / (max_X - min_X)\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n",
        "    datasets[dataset_name] = (X_train, y_train, X_test, y_test)  \n",
        "    print(np.sum(min_X))\n",
        "    print(np.sum(max_X))\n",
        "    print(dataset_name)  \n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "\n",
        "# MNIST\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "add_dataset(X_train, y_train, X_test, y_test, \"MNIST\")\n",
        "\n",
        "# USPS\n",
        "mat = io.loadmat(path+\"/Images_Datasets/USPS.mat\")\n",
        "X_train = X_test = mat['X'].reshape( (mat['X'].shape[0], int(sqrt(mat['X'].shape[1])), int(sqrt(mat['X'].shape[1]))) )\n",
        "y_train = y_test = mat['y']\n",
        "add_dataset(X_train, y_train, X_test, y_test, \"USPS\")   \n",
        "\n",
        "# COIL 20\n",
        "mat = io.loadmat(path+\"/Images_Datasets/coil_20.mat\")\n",
        "mat['X'] = np.array(mat['X'].todense())\n",
        "X_train = X_test = mat['X'].reshape( (mat['X'].shape[0], int(sqrt(mat['X'].shape[1])), int(sqrt(mat['X'].shape[1]))) )\n",
        "y_train = y_test = np.repeat(np.arange(20), 72)\n",
        "add_dataset(X_train, y_train, X_test, y_test, \"COIL-20\")   \n",
        "\n",
        "# COIL 100\n",
        "mat = io.loadmat(path+\"/Images_Datasets/coil_100.mat\")\n",
        "mat['X'] = np.array(mat['X'].todense())\n",
        "X_train = X_test = mat['X'].reshape( (mat['X'].shape[0], int(sqrt(mat['X'].shape[1])), int(sqrt(mat['X'].shape[1]))) )\n",
        "y_train = y_test = np.repeat(np.arange(100), 72)\n",
        "add_dataset(X_train, y_train, X_test, y_test, \"COIL-100\")   \n",
        "\n",
        "# ORL\n",
        "mat = io.loadmat(path+\"/Images_Datasets/orl.mat\")\n",
        "mat['X'] = np.array(mat['X'].todense())\n",
        "X_train = X_test = mat['X'].reshape( (mat['X'].shape[0], 112, 92) )\n",
        "y_train = y_test = np.repeat(np.arange(40), 10)\n",
        "add_dataset(X_train, y_train, X_test, y_test, \"ORL\")   \n",
        "\n",
        "\n",
        "#################################################\n",
        "# Functions\n",
        "#################################################\n",
        "num_examples_to_generate=16\n",
        "def plot_images(predictions, epoch):\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 255.0, cmap='gray')\n",
        "      #plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  plot_images(predictions, epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EV8pt0X4xWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "X, y, _, _ = datasets[\"MNIST\"]\n",
        "\n",
        "gmm = GaussianMixture(n_components=len(np.unique(y)), n_init=20, max_iter=300, verbose=1)\n",
        "label = gmm.fit_predict(X.reshape((X.shape[0], X.shape[1]*X.shape[2])))\n",
        "\n",
        "res_nmi = nmi(label, y)\n",
        "res_ari = ari(label, y)       \n",
        "print(res_nmi)\n",
        "print(res_ari)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liFWbx7orbkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "X_train, y_train, _, _ = datasets[\"MNIST\"]\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, n_init=20)\n",
        "kmeans.fit(X.reshape((X.shape[0], X.shape[1]*X.shape[2])))\n",
        "\n",
        "print(nmi(kmeans.labels_, y_train))\n",
        "print(ari(kmeans.labels_, y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m913TteujpvI",
        "colab_type": "text"
      },
      "source": [
        "## **1. Deep Autoencoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7cPBvxyy5xW",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuiQnKkSy-fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import Input, layers\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Hyper-parameters\n",
        "#################################################\n",
        "epoch = 100\n",
        "batch_size = 256\n",
        "\n",
        "encoding_dim = 7*7\n",
        "hidden_dim = 512\n",
        "\n",
        "#activation='elu'\n",
        "activation='leaky_relu'\n",
        "optimizer='adam'\n",
        "#loss='mean_squared_error'\n",
        "loss='binary_crossentropy'\n",
        "#################################################\n",
        "\n",
        "out_activation = 'sigmoid' if loss=='binary_crossentropy' else 'linear'\n",
        "activation = None if activation=='leaky_relu' else activation\n",
        "\n",
        "def make_autoencoder_model():\n",
        "    #################################################\n",
        "    # Auto-encoder\n",
        "    #################################################\n",
        "    input_img = Input(shape=(784,))\n",
        "\n",
        "\n",
        "    ####### Encoder #######\n",
        "    #   Layer\n",
        "    encoded = Dense(hidden_dim, activation=activation)(input_img)\n",
        "    if activation is None: encoded = layers.LeakyReLU()(encoded)\n",
        "\n",
        "    #   Layer\n",
        "    encoded = Dense(hidden_dim//4, activation=activation)(encoded)\n",
        "    if activation is None: encoded = layers.LeakyReLU()(encoded)\n",
        "\n",
        "    #   Layer\n",
        "    encoded = Dense(encoding_dim, activation=out_activation)(encoded)\n",
        "\n",
        "\n",
        "    ####### Decoder #######\n",
        "    #   Layer\n",
        "    decoded = Dense(hidden_dim//4, activation=activation)(encoded)\n",
        "    if activation is None: decoded = layers.LeakyReLU()(decoded)\n",
        "\n",
        "    #   Layer\n",
        "    decoded = Dense(hidden_dim, activation=activation)(decoded)\n",
        "    if activation is None: decoded = layers.LeakyReLU()(decoded)\n",
        "\n",
        "    #   Layer\n",
        "    decoded = Dense(784, activation=out_activation)(decoded)\n",
        "\n",
        "\n",
        "    ####### Make model #######\n",
        "    autoencoder = Model(input_img, decoded)\n",
        "\n",
        "    return autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT83MRLly6Ns",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t4xn_WDhH6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "autoencoder = make_autoencoder_model()\n",
        "autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "autoencoder.summary()\n",
        "####### Train #######\n",
        "autoencoder.fit(X_train, X_train,\n",
        "                epochs=epoch,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, X_test))\n",
        "\n",
        "\n",
        "####### Encode images #######\n",
        "encoder = Model(input_img, encoded)\n",
        "encoded_images = encoder.predict(X_train)\n",
        "#################################################\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# KMeans\n",
        "#################################################\n",
        "print(\"run KMeans\")\n",
        "kmeans = KMeans(n_clusters=10, n_init=20)\n",
        "kmeans.fit(encoded_images)\n",
        "\n",
        "print(nmi(kmeans.labels_, y_train))\n",
        "print(ari(kmeans.labels_, y_train))\n",
        "#################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJll9Pazj4dD",
        "colab_type": "text"
      },
      "source": [
        "## **2. Deep Convolutional Autoencoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pjWP6FRuSwF",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5_qJyYio4CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import Input, layers, models\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "from math import sqrt\n",
        "\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Hyper-parameters\n",
        "#################################################\n",
        "epoch = 10\n",
        "batch_size = 256\n",
        "\n",
        "#activation='elu'\n",
        "activation='leaky_relu'\n",
        "optimizer='adam'\n",
        "#loss='mean_squared_error'\n",
        "loss='binary_crossentropy'\n",
        "num_conv=2\n",
        "\n",
        "use_batch_norm_encoded = False\n",
        "use_batch_norm_decoded = False\n",
        "#################################################\n",
        "\n",
        "out_activation = 'sigmoid' if loss=='binary_crossentropy' else 'linear'\n",
        "activation = None if activation=='leaky_relu' else activation\n",
        "\n",
        "\n",
        "def conv_layer(input_tensor, filters=32, kernel=(3,3), strides=1, \n",
        "               activation=\"relu\", use_batch_norm=False, padding='valid'):\n",
        "  out = layers.Conv2D(filters, kernel, strides=strides, activation=activation, padding=padding)(input_tensor)\n",
        "  if use_batch_norm:     out = layers.BatchNormalization()(out)\n",
        "  if activation is None: out = layers.LeakyReLU()(out)\n",
        "  return out\n",
        "\n",
        "def deconv_layer(input_tensor, filters=32, kernel=(3,3), strides=1, \n",
        "                 activation=\"relu\", use_batch_norm=False, padding='valid'):\n",
        "  out = layers.Conv2DTranspose(filters, kernel, strides=strides, activation=activation, padding=padding)(input_tensor)\n",
        "  if use_batch_norm:     out = layers.BatchNormalization()(out)\n",
        "  if activation is None: out = layers.LeakyReLU()(out)\n",
        "  return out\n",
        "\n",
        "\n",
        "def make_conv_autoencoder_model(img_height=28, img_width=28):\n",
        "  big_img = img_height >= 18 and img_width >= 18\n",
        "  big_big_img = img_height >= 64 and img_width >= 64\n",
        "  divisor = 4 if big_img else 2\n",
        "  encoding_height, encoding_width = img_height//divisor, img_width//divisor\n",
        "  encoding_dim = encoding_height * encoding_width\n",
        "\n",
        "  #################################################\n",
        "  # Convolutional Auto-encoder\n",
        "  #################################################\n",
        "  input_img = Input((img_height, img_width, 1))\n",
        "\n",
        "  ####### Encoder #######\n",
        "  encoded = input_img\n",
        "  #   Layer\n",
        "  for _ in range(num_conv):\n",
        "      encoded = conv_layer(encoded, filters=32, kernel=(3,3), strides=1, \n",
        "                          activation=activation, use_batch_norm=use_batch_norm_encoded,\n",
        "                          padding='same')  \n",
        "  encoded = layers.MaxPooling2D((2, 2))(encoded)\n",
        "\n",
        "  #   Layer\n",
        "  for _ in range(num_conv):\n",
        "      encoded = conv_layer(encoded, filters=64, kernel=(3,3), strides=1, \n",
        "                          activation=activation, use_batch_norm=use_batch_norm_encoded,\n",
        "                          padding='same')  \n",
        "  if big_img: encoded = layers.MaxPooling2D((2, 2))(encoded) # otherwise we will have dimension issues\n",
        "\n",
        "  #   Layer\n",
        "  for _ in range(num_conv-1):\n",
        "      encoded = conv_layer(encoded, filters=64, kernel=(3,3), strides=1, \n",
        "                          activation=activation, use_batch_norm=use_batch_norm_encoded,\n",
        "                          padding='same')\n",
        "  encoded = conv_layer(encoded, filters=64, kernel=(3,3), strides=1, \n",
        "                      activation=activation, use_batch_norm=use_batch_norm_encoded)\n",
        "\n",
        "  #   Layer\n",
        "  encoded = layers.Flatten()(encoded)\n",
        "  encoded = layers.Dense(encoding_dim, activation=out_activation)(encoded)\n",
        "\n",
        "\n",
        "  ####### Decoder #######\n",
        "  decoded_input = encoded if encoding_dim == encoding_height * encoding_width else layers.Dense(encoding_height * encoding_width)(encoded)\n",
        "  decoded = layers.Reshape((encoding_height, encoding_width, 1))(decoded_input)\n",
        "\n",
        "  #   Layer\n",
        "  strides = 2 if big_img else 1\n",
        "  decoded = deconv_layer(decoded, filters=128, kernel=(3,3), strides=strides, \n",
        "                         activation=activation, use_batch_norm=use_batch_norm_decoded,\n",
        "                         padding='same')\n",
        "  for _ in range(num_conv-1):\n",
        "      decoded = deconv_layer(decoded, filters=128, kernel=(3,3), strides=1, \n",
        "                          activation=activation, use_batch_norm=use_batch_norm_decoded,\n",
        "                          padding='same')\n",
        "\n",
        "  #   Layer\n",
        "  decoded = deconv_layer(decoded, filters=64, kernel=(3,3), strides=2, \n",
        "                         activation=activation, use_batch_norm=use_batch_norm_decoded,\n",
        "                         padding='same')  \n",
        "  for _ in range(num_conv-1):\n",
        "      decoded = deconv_layer(decoded, filters=64, kernel=(3,3), strides=1, \n",
        "                          activation=activation, use_batch_norm=use_batch_norm_decoded,\n",
        "                          padding='same')\n",
        "\n",
        "  #   Layer\n",
        "  for _ in range(num_conv):\n",
        "      decoded = deconv_layer(decoded, filters=32, kernel=(3,3), strides=1, \n",
        "                            activation=activation, use_batch_norm=use_batch_norm_decoded,\n",
        "                            padding='same')\n",
        "\n",
        "  #   Layer\n",
        "  decoded = layers.Conv2D(1, (3, 3), activation=out_activation, padding='same')(decoded)\n",
        "\n",
        "\n",
        "  ####### Make model #######\n",
        "  autoencoder = models.Model(input_img, decoded)\n",
        "  encoder = models.Model(input_img, encoded)\n",
        "  #decoder = models.Model(encoded, decoded)\n",
        "  autoencoder.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "  return autoencoder, encoder, decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7PX9eVuqqI",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqLpAQewhml1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = 100\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "autoencoder, encoder, decoded = make_conv_autoencoder_model(X_train.shape[1], X_train.shape[2])\n",
        "autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "autoencoder.summary()\n",
        "\n",
        "\"\"\"ae_checkpoint_dir = path+'/ae_training_checkpoints'\n",
        "ae_checkpoint_prefix = os.path.join(ae_checkpoint_dir, \"ckpt\")\n",
        "ae_checkpoint = tf.train.Checkpoint(autoencoder=autoencoder)\n",
        "\n",
        "ae_checkpoint.restore(tf.train.latest_checkpoint(ae_checkpoint_dir))\"\"\"\n",
        "\n",
        "\"\"\"for _ in range(8):\n",
        "  generate_and_save_images(autoencoder, 0, \n",
        "                        test_input=X_train[np.random.randint(low=0,high=train_images.shape[0],size=16)])\"\"\"\n",
        "\n",
        "####### Train #######\n",
        "autoencoder.fit(X_train, X_train,\n",
        "                epochs=epoch,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, X_test))\n",
        "\n",
        "#ae_checkpoint.save(file_prefix = ae_checkpoint_prefix)\n",
        "\n",
        "for _ in range(8):\n",
        "  generate_and_save_images(autoencoder, 0, \n",
        "                        test_input=X_train[np.random.randint(low=0,high=X_train.shape[0],size=16)])\n",
        "\n",
        "#################################################\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# KMeans\n",
        "#################################################\n",
        "####### Encode images #######\n",
        "encoded_images = encoder.predict(X_train)\n",
        "print(encoded_images.shape)\n",
        "\n",
        "####### KMeans #######\n",
        "print(\"run KMeans\")\n",
        "kmeans = KMeans(n_clusters=10, n_init=20)\n",
        "kmeans.fit(encoded_images)\n",
        "\n",
        "print(nmi(kmeans.labels_, y_train.ravel()))\n",
        "print(ari(kmeans.labels_, y_train.ravel()))\n",
        "#################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzIB_EMszFFp",
        "colab_type": "text"
      },
      "source": [
        "## **3. LLE**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sju6W-fY7I4B",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIJQbmP94cB",
        "colab_type": "text"
      },
      "source": [
        "\\begin{equation}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_i} = 2G_i w_i - \\lambda \\mathbf{1} \\\\\n",
        "\\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = \\mathbf{1}^T w_i - \\mathbf{1}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXaGt_Hr30bf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from scipy.linalg import solve, eigh\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "from sklearn.utils import check_random_state\n",
        "from scipy.sparse.linalg import eigsh\n",
        "\n",
        "\n",
        "\n",
        "class LLE():\n",
        "    def __init__(self, n_neighbors=5, n_components=2, n_jobs=None, verbose=True, \n",
        "                 learning_rate=0.0001, neighbors_update=False, method=\"direct\",\n",
        "                 reg=1e-3):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.n_components = n_components\n",
        "        self.n_jobs = n_jobs\n",
        "        self.verbose = verbose        \n",
        "        self.learning_rate = learning_rate        # Used only if method=\"gradient\"\n",
        "        self.neighbors_update = neighbors_update\n",
        "        self.method = method\n",
        "        self.reg = reg\n",
        "\n",
        "        self.W = None                            # Sparse full Weight matrix [n_samples x n_samples]\n",
        "        self.B = None                            # Reduced Weight matrix [n_samples x n_neighbors]\n",
        "        self.lagrange_lambda = 0                 # Lambda (lagrange multiplier)\n",
        "        self.knn = None                          # Store kNN result to avoid computation each time fit is called\n",
        "        self.ind = None                          # Store kNN neighbors indexes\n",
        "\n",
        "    def _update_W(self, regularization=True):\n",
        "        n_samples = self.B.shape[0]\n",
        "        indptr = np.arange(0, n_samples * self.n_neighbors + 1, self.n_neighbors)\n",
        "        rowsum = self.B.sum(1)[:,None]\n",
        "        rowsum[rowsum == 0] = 1 \n",
        "        data = self.B \n",
        "        if regularization:\n",
        "            data = data / rowsum\n",
        "        self.W = sparse.csr_matrix((data.ravel(), self.ind.ravel(), indptr),\n",
        "                      shape=(n_samples, n_samples))\n",
        "\n",
        "    def get_W(self):\n",
        "        self._update_W()\n",
        "        return self.W\n",
        "\n",
        "    def get_M(self):\n",
        "        W = self.get_W()\n",
        "        M = sparse.eye(*W.shape) - W\n",
        "        M = M.T.dot(M)\n",
        "        return M\n",
        "\n",
        "    def get_WX(self, X, to_fit=True):\n",
        "        self.fit(X)\n",
        "        W = self.get_W()\n",
        "        X_csr = sparse.csr_matrix(X)\n",
        "        return W.dot(X_csr)\n",
        "\n",
        "    # From sklearn's \"null_space\" function\n",
        "    # https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/manifold/_locally_linear.py\n",
        "    def get_Y(self, k_skip=1, eigen_solver='arpack', tol=1E-6, max_iter=100,\n",
        "            random_state=None):\n",
        "        M = self.get_M()\n",
        "        k = self.n_components\n",
        "        if eigen_solver == 'auto':\n",
        "            if M.shape[0] > 200 and k + k_skip < 10:\n",
        "                eigen_solver = 'arpack'\n",
        "            else:\n",
        "                eigen_solver = 'dense'\n",
        "\n",
        "        if eigen_solver == 'arpack':\n",
        "            random_state = check_random_state(random_state)\n",
        "            # initialize with [-1,1] as in ARPACK\n",
        "            v0 = random_state.uniform(-1, 1, M.shape[0])\n",
        "            try:\n",
        "                eigen_values, eigen_vectors = eigsh(M, k + k_skip, sigma=0.0,\n",
        "                                                    tol=tol, maxiter=max_iter,\n",
        "                                                    v0=v0)\n",
        "            except RuntimeError as msg:\n",
        "                print(\"arpack RuntimeError\")\n",
        "                return self.get_Y(k_skip=k_skip, eigen_solver=\"dense\", tol=tol, max_iter=max_iter)\n",
        "                \n",
        "            return eigen_vectors[:, k_skip:], np.sum(eigen_values[k_skip:])\n",
        "        elif eigen_solver == 'dense':\n",
        "            if hasattr(M, 'toarray'):\n",
        "                M = M.toarray()\n",
        "            eigen_values, eigen_vectors = eigh(\n",
        "                M, eigvals=(k_skip, k + k_skip - 1), overwrite_a=True)\n",
        "            index = np.argsort(np.abs(eigen_values))\n",
        "            return eigen_vectors[:, index], np.sum(eigen_values)\n",
        "        else:\n",
        "            raise ValueError(\"Unrecognized eigen_solver '%s'\" % eigen_solver)        \n",
        "\n",
        "    def compute_weight_loss(self, X):\n",
        "        W = self.get_W()\n",
        "        X_csr = sparse.csr_matrix(X)\n",
        "        #return np.square(X - np.dot(W.todense(), X)).sum() # Too computationnaly expansive to work on dense matrices\n",
        "        return (X_csr - W.dot(X_csr)).power(2).sum()\n",
        "\n",
        "    def _one_fit(self, X):\n",
        "        Z = X[self.ind]\n",
        "        n_samples, n_neighbors = X.shape[0], Z.shape[1]\n",
        "        ones = np.ones(n_neighbors)\n",
        "\n",
        "        for i, A in enumerate(Z.transpose(0, 2, 1)):\n",
        "            #C = A.T - X[i]  \n",
        "            C = X[i] - A.T\n",
        "            G = np.dot(C, C.T)\n",
        "\n",
        "            if self.method == \"gradient\":\n",
        "                # Gradient descent way\n",
        "                w_gradient = 2 * np.dot(G, self.B[i, :]) - self.lagrange_lambda * ones\n",
        "                lambda_gradient = np.dot(ones, self.B[i, :]) - 1\n",
        "\n",
        "                self.lagrange_lambda -= self.learning_rate * lambda_gradient\n",
        "                self.B[i, :] -= self.learning_rate * w_gradient\n",
        "\n",
        "                b_sum = self.B[i, :].sum()\n",
        "                if b_sum != 0: \n",
        "                    self.B[i, :] = self.B[i, :] / b_sum \n",
        "\n",
        "            elif self.method == \"direct\":\n",
        "                # Direct way, from sklearn's \"barycenter_weights\" function\n",
        "                # https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/manifold/_locally_linear.py\n",
        "                trace = np.trace(G)\n",
        "                if trace > 0:\n",
        "                    R = self.reg * trace\n",
        "                else:\n",
        "                    R = self.reg\n",
        "                G.flat[::Z.shape[1] + 1] += R\n",
        "                w = solve(G, ones, sym_pos=True)\n",
        "                self.B[i, :] = w / np.sum(w)\n",
        "\n",
        "\n",
        "    def fit(self, X, epoch=1):\n",
        "        # Run kNN algorithm      \n",
        "        if self.knn is None or self.neighbors_update:\n",
        "            if self.verbose: print(\"Run kNN...\")\n",
        "            self.knn = NearestNeighbors(self.n_neighbors + 1, n_jobs=self.n_jobs).fit(X)\n",
        "            self.ind = self.knn.kneighbors(X, return_distance=False)[:, 1:]\n",
        "        \n",
        "        # Init B matrix\n",
        "        if self.B is None:\n",
        "            \"\"\"self.B = np.random.random((X.shape[0], self.n_neighbors))\n",
        "            self.B /= self.B.sum(1)[:,None]\"\"\"\n",
        "            self.B = np.zeros((X.shape[0], self.n_neighbors))\n",
        "\n",
        "        # Train\n",
        "        if self.verbose: \n",
        "            print(\"Train...\")\n",
        "            #print(\"LLE loss = \", self.compute_weight_loss(X))\n",
        "        for ep in range(epoch):\n",
        "            t0 = time()\n",
        "            self._one_fit(X)\n",
        "            t1 = time()\n",
        "            if self.verbose:\n",
        "                print(\": %.2g sec\" % (t1 - t0))\n",
        "            if self.method != \"direct\": \n",
        "                if ep % 100 == 0 and self.verbose:\n",
        "                    print(ep, \" \\ \", epoch)\n",
        "                    print(\"LLE loss = \", self.compute_weight_loss(X))\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"LLE loss = \", self.compute_weight_loss(X))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return None\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5tSJxbb7LIJ",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8qdLOw27PxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\n",
        "X = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
        "print(X.shape)\n",
        "\n",
        "#lle = LLE(method=\"gradient\")\n",
        "#lle.fit(X, epoch=1000)\n",
        "\n",
        "lle = LLE()\n",
        "lle.fit(X)\n",
        "\n",
        "Y, error = lle.get_Y()\n",
        "print(Y.shape)\n",
        "print(error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpNfGS_Z3taI",
        "colab_type": "text"
      },
      "source": [
        "## **4. AE LLE**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y66gU3n_I4Eg",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JZMyr1kI7LT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from scipy import sparse\n",
        "from time import time\n",
        "\n",
        "\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "class AE_LLE():\n",
        "    def __init__(self, autoencoder, encoder, LLE, batch_size=128, mode=\"full\", l_reg=1.00):\n",
        "        \"\"\"mode : {\"batch\", \"full\"} \n",
        "                  be careful, LLE in batch mode will not take the full dataset \n",
        "                  in input hence its global structure\"\"\"\n",
        "        self.autoencoder = autoencoder\n",
        "        self.encoder = encoder\n",
        "        self.lle = LLE\n",
        "        self.batch_size = batch_size\n",
        "        self.ae_lle_optimizer = self.make_ae_lle_optimizer()\n",
        "        self.mode = mode\n",
        "        self.l_reg = l_reg\n",
        "\n",
        "    def get_X_encoded(self, X):\n",
        "        return self.encoder.predict(X)\n",
        "\n",
        "    def get_Y_lle(self):\n",
        "        return self.lle.get_Y()\n",
        "\n",
        "    def encoder_loss(self, X_encoded, W, use_sparse=False):\n",
        "        if use_sparse:\n",
        "            X_encoded_sparse = sparse.csr_matrix(X_encoded)\n",
        "            WX = W.dot(X_encoded_sparse).todense()\n",
        "        else:\n",
        "            #WX = W.dot(X_encoded)            \n",
        "            WX = tf.matmul(tf.convert_to_tensor(W, dtype=tf.float32), X_encoded)\n",
        "        return self.l_reg * mse(X_encoded, WX)\n",
        "\n",
        "    def ae_lle_loss(self, X, X_decoded, X_encoded, W):\n",
        "        return cross_entropy(X, X_decoded) + self.l_reg * self.encoder_loss(X_encoded, W)\n",
        "\n",
        "    def make_ae_lle_optimizer(self):\n",
        "        return tf.keras.optimizers.Adam()\n",
        "        #return tf.keras.optimizers.Adam(2e-3)\n",
        "\n",
        "    # Train encoder according to the second term of the loss\n",
        "    @tf.function\n",
        "    def ae_lle_train_step(self, X, W, use_sparse=False, train_both=False):\n",
        "        with tf.GradientTape() as ae_lle_tape:\n",
        "          X_encoded = self.encoder(X, training=True)\n",
        "          if not train_both:\n",
        "              loss = self.encoder_loss(X_encoded, W, use_sparse=use_sparse)\n",
        "          else:\n",
        "              X_decoded = self.autoencoder(X, training=True)\n",
        "              loss = self.ae_lle_loss(X, X_decoded, X_encoded, W)\n",
        "\n",
        "        if not train_both:\n",
        "            gradients_of_ae_lle = ae_lle_tape.gradient(loss, self.encoder.trainable_variables)\n",
        "            self.ae_lle_optimizer.apply_gradients(zip(gradients_of_ae_lle, self.encoder.trainable_variables))\n",
        "        else:\n",
        "            gradients_of_ae_lle = ae_lle_tape.gradient(loss, self.autoencoder.trainable_variables)\n",
        "            self.ae_lle_optimizer.apply_gradients(zip(gradients_of_ae_lle, self.autoencoder.trainable_variables))          \n",
        "\n",
        "        return loss\n",
        "\n",
        "    def fit(self, X, epoch=50, ae_step=10):\n",
        "        t0 = time()\n",
        "        if self.mode == \"batch\":\n",
        "            epoch = epoch * batch_size\n",
        "        ####### Train #######\n",
        "        for ep in range(epoch):\n",
        "            X_batch = X\n",
        "            if self.mode == \"batch\":\n",
        "                indexes = np.random.randint(low=0,high=X.shape[0],size=batch_size)\n",
        "                X_batch = X[indexes]\n",
        "\n",
        "            for i in range(ae_step):                \n",
        "                # LLE Training\n",
        "                if i == 0: # fit only on the first ae_step\n",
        "                    #print(\"--- LLE update...\")\n",
        "                    X_encoded = self.encoder.predict(X_batch)\n",
        "                    #print(X_encoded.shape)\n",
        "                    self.lle.fit(X_encoded)\n",
        "                    W = self.lle.get_W()\n",
        "\n",
        "                # Encoder Training\n",
        "                #print(\"--- Encoder update...\")\n",
        "                #encoder_loss = self.ae_lle_train_step(X_batch, W.todense(), train_both=True)\n",
        "                encoder_loss = self.ae_lle_train_step(X_batch, W.todense())\n",
        "                #encoder_loss = self.ae_lle_train_step(X_batch, W, use_sparse=True)\n",
        "\n",
        "                # Autoencoder Training\n",
        "                #print(\"--- Autoencoder update...\")\n",
        "                history = self.autoencoder.fit(X_batch, X_batch,\n",
        "                                epochs=1,\n",
        "                                batch_size=self.batch_size,\n",
        "                                shuffle=True, verbose=0)\n",
        "                results = history.history['loss'][0]\n",
        "                #results = self.autoencoder.evaluate(X, X, batch_size=self.batch_size, verbose=0)    \n",
        "\n",
        "            if ep % (epoch//10) == 0:\n",
        "                t1 = time()\n",
        "                print(\"_________________________________________________________________\")\n",
        "                print(\"epoch : \",ep, \" / \", epoch, \"(%.2g sec)\"%(t1 - t0))\n",
        "                print(\"Autoencoder loss = \", results)             \n",
        "                print(\"LLE loss         = \", self.lle.compute_weight_loss(X_encoded))            \n",
        "                print(\"Encoder loss     = \", encoder_loss.numpy())\n",
        "\n",
        "        \"\"\"for _ in range(8):\n",
        "          generate_and_save_images(self.autoencoder, 0, \n",
        "                                test_input=X[np.random.randint(low=0,high=X.shape[0],size=16)])\"\"\"\n",
        "\n",
        "        X_encoded = self.encoder.predict(X)\n",
        "        self.lle.fit(X_encoded)\n",
        "\n",
        "        print(\"Autoencoder loss = \", results)             \n",
        "        print(\"LLE loss         = \", self.lle.compute_weight_loss(X_encoded))            \n",
        "        print(\"Encoder loss     = \", encoder_loss.numpy())\n",
        "\n",
        "    def transform(self, X):\n",
        "        pass     \n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        pass           \n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "encoder_losses = []\n",
        "# Batch Encoder Training\n",
        "for it in range(int(X_train.shape[0] / batch_size)):\n",
        "    indexes = np.random.randint(low=0,high=X_train.shape[0],size=batch_size)\n",
        "    X_batch = X_train[indexes]  \n",
        "    W_batch = W[indexes]\n",
        "    print(W_batch.shape)\n",
        "    print(X_batch.shape)\n",
        "    input()\n",
        "    encoder_loss = ae_lle_train_step(X_batch, W_batch)\n",
        "    encoder_losses.append(encoder_loss)\n",
        "print(\"Encoder loss = \", mean(encoder_losses))\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pOYva_VI4JO",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYt1DqAqoXkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = 50\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from statistics import mean\n",
        "\n",
        "autoencoder, encoder, decoded = make_conv_autoencoder_model(X_train.shape[1], X_train.shape[2])\n",
        "autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "autoencoder.summary()\n",
        "\n",
        "lle = LLE(neighbors_update=True, verbose=False) # we need to recompute neighbors at each iteration autoencoder encoding changes over time\n",
        "\n",
        "ae_lle = AE_LLE(autoencoder, encoder, lle)\n",
        "ae_lle.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOUxqpcG6RA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"ae_checkpoint_dir = path+'/ae_training_checkpoints'\n",
        "ae_checkpoint_prefix = os.path.join(ae_checkpoint_dir, \"ckpt\")\n",
        "ae_checkpoint = tf.train.Checkpoint(autoencoder=autoencoder)\n",
        "\n",
        "ae_checkpoint.restore(tf.train.latest_checkpoint(ae_checkpoint_dir))\n",
        "\n",
        "ae_checkpoint.save(file_prefix = ae_checkpoint_prefix)\"\"\"\n",
        "\n",
        "\"\"\"for _ in range(8):\n",
        "  generate_and_save_images(autoencoder, 0, \n",
        "                        test_input=X_train[np.random.randint(low=0,high=train_images.shape[0],size=16)])\"\"\"\n",
        "\n",
        "\"\"\"for it in range(int(X_train.shape[0] / batch_size)):\n",
        "    X_batch = X_train[np.random.randint(low=0,high=X_train.shape[0],size=batch_size)]  \n",
        "    autoencoder.fit(X_batch, X_batch,\n",
        "                    epochs=1,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True, verbose=0)\n",
        "    \n",
        "results = autoencoder.evaluate(X_test, X_test, batch_size=batch_size)\n",
        "print('Autoencoder validation:', results)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnW1wqGu4ZVr",
        "colab_type": "text"
      },
      "source": [
        "#### FCPS Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69I7jIfB6NbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fcps_run import execute_algo, add_subplot\n",
        "from fcps_autoencoder import Autoencoder\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "from scipy import io\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import manifold\n",
        "from statistics import mean, stdev\n",
        "\n",
        "from time import time  \n",
        "\n",
        "\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "dataset_folder = path+\"/FCPS\"\n",
        "\n",
        "subplot_row = 1\n",
        "subplot_col = 8\n",
        "\n",
        "\n",
        "def run_kMeans(X, y, name, algo=2):\n",
        "    if algo == 1:\n",
        "      kmeans = KMeans(n_clusters=len(np.unique(y)), n_init=20, max_iter=300)\n",
        "      kmeans.fit(X)\n",
        "      label = kmeans.labels_\n",
        "    elif algo == 2:\n",
        "      gmm = GaussianMixture(n_components=len(np.unique(y)), n_init=20, max_iter=300)\n",
        "      label = gmm.fit_predict(X)\n",
        "    res_nmi = nmi(label, y)\n",
        "    res_ari = ari(label, y)      \n",
        "    print(name)    \n",
        "    print(res_nmi)\n",
        "    print(res_ari)\n",
        "    return res_nmi, res_ari\n",
        "\n",
        "\n",
        "todo = [\"Lsun.mat\", \"Target.mat\" \"Tetra.mat\", \"TwoDiamonds.mat\", \"WingNut.mat\"]\n",
        "all_files = os.listdir(dataset_folder)\n",
        "\n",
        "mode = 1\n",
        "it = 10\n",
        "\n",
        "for myfile in all_files:\n",
        "    if mode >= 2 and myfile not in todo: continue\n",
        "    \n",
        "    print(\"#####################\")\n",
        "    print(\"#\",myfile)\n",
        "    print(\"#####################\")\n",
        "    mat = io.loadmat(dataset_folder+\"/\"+myfile)\n",
        "    X = mat[\"fea\"]\n",
        "    y = mat[\"gnd\"]\n",
        "    print(X.shape)\n",
        "    print(y.shape)\n",
        "\n",
        "    X_norm = (X - X.min(0))/(X.max(0) - X.min(0))\n",
        "    \n",
        "    gmm_nmis, gmm_aris, lle_nmis, lle_aris, ae_nmis, ae_aris, ae_lle_e_nmis, ae_lle_e_aris, ae_lle_l_nmis, ae_lle_l_aris = [], [], [], [], [], [], [], [], [], []\n",
        "    best_mean_nmi, best_mean_ari = 0, 0\n",
        "    for i in range(it):\n",
        "        print(\"_________________________________________________________________\")\n",
        "        print(i, \" \\ \", it)\n",
        "        temp = open(path+\"/Results/temp.txt\", \"w\")\n",
        "        temp.write(str(i))\n",
        "        temp.close()\n",
        "\n",
        "        subplot_cpt = 1\n",
        "        \n",
        "        #####################\n",
        "        # Make figure\n",
        "        #####################\n",
        "        fig = plt.figure(figsize=(15, 5))\n",
        "        \n",
        "        \n",
        "        #####################    \n",
        "        # Locally Linear Embedding\n",
        "        #####################    \n",
        "        #subplot_cpt += 2\n",
        "        X_lle = execute_algo(manifold.LocallyLinearEmbedding, X, \"LLE\", subplot_cpt, y, fig)\n",
        "        \n",
        "\n",
        "        #####################    \n",
        "        # Autoencoder\n",
        "        #####################         \n",
        "        subplot_cpt += 2 \n",
        "        X_ae = execute_algo(Autoencoder, X, \"Autoencoder\", subplot_cpt, y, fig)\n",
        "\n",
        "\n",
        "        #####################    \n",
        "        # AE LLE\n",
        "        #####################\n",
        "        subplot_cpt += 2 \n",
        "        print(\"AE LLE...\")        \n",
        "        t0 = time()        \n",
        "        autoencoder, encoder, decoded = Autoencoder().make_autoencoder_model(X.shape[1])\n",
        "        lle = LLE(neighbors_update=True, verbose=False)\n",
        "        ae_lle = AE_LLE(autoencoder, encoder, lle)\n",
        "        ae_lle.fit(X_norm, epoch=300)        \n",
        "        X_ae_lle_X_encoded = ae_lle.get_X_encoded(X_norm)\n",
        "        X_ae_lle_Y_lle, _ = ae_lle.get_Y_lle()\n",
        "        t1 = time()\n",
        "        print(\"AE LLE : %.2g sec\" % (t1 - t0))\n",
        "        add_subplot(\"AE LLE (X encoded)\", subplot_cpt, y, fig, t0, t1, X_ae_lle_X_encoded)\n",
        "        subplot_cpt += 2\n",
        "        add_subplot(\"AE LLE (Y lle)\", subplot_cpt, y, fig, t0, t1, X_ae_lle_Y_lle)\n",
        "\n",
        "\n",
        "        #####################    \n",
        "        # k-Means\n",
        "        #####################\n",
        "        tot_nmi, tot_ari = 0, 0        \n",
        "        # Classical kMeans    \n",
        "        \"\"\"res_nmi, res_ari = run_kMeans(X, y.ravel(), \"GMM\")\n",
        "        gmm_nmis.append(res_nmi)\n",
        "        gmm_aris.append(res_ari)\n",
        "        tot_nmi, tot_ari = tot_nmi + res_nmi, tot_ari + res_ari\"\"\"\n",
        "        \n",
        "        # kMeans on LLE\n",
        "        res_nmi, res_ari = run_kMeans(X_lle, y.ravel(), \"LLE\")\n",
        "        lle_nmis.append(res_nmi)\n",
        "        lle_aris.append(res_ari)        \n",
        "        tot_nmi, tot_ari = tot_nmi + res_nmi, tot_ari + res_ari\n",
        "        \n",
        "        # kMeans on AE\n",
        "        res_nmi, res_ari = run_kMeans(X_ae, y.ravel(), \"AE\")\n",
        "        ae_nmis.append(res_nmi)\n",
        "        ae_aris.append(res_ari)       \n",
        "        tot_nmi, tot_ari = tot_nmi + res_nmi, tot_ari + res_ari \n",
        "        \n",
        "        # kMeans on AE LLE\n",
        "        res_nmi, res_ari = run_kMeans(X_ae_lle_X_encoded, y.ravel(), \"AE LLE (X encoded)\")\n",
        "        ae_lle_e_nmis.append(res_nmi)\n",
        "        ae_lle_e_aris.append(res_ari)   \n",
        "        tot_nmi, tot_ari = tot_nmi + res_nmi, tot_ari + res_ari     \n",
        "\n",
        "        # kMeans on AE LLE\n",
        "        res_nmi, res_ari = run_kMeans(X_ae_lle_Y_lle, y.ravel(), \"AE LLE (Y lle)\")\n",
        "        ae_lle_l_nmis.append(res_nmi)\n",
        "        ae_lle_l_aris.append(res_ari)    \n",
        "        tot_nmi, tot_ari = tot_nmi + res_nmi, tot_ari + res_ari    \n",
        "        \n",
        "        #####################      \n",
        "        # Final plot\n",
        "        #####################      \n",
        "        #plt.show()\n",
        "        mean_nmi, mean_ari = tot_nmi/4, tot_ari/4\n",
        "        if mean([mean_nmi, mean_ari]) > mean([best_mean_nmi, best_mean_ari]):\n",
        "            best_mean_nmi, best_mean_ari = mean_nmi, mean_ari\n",
        "            plt.subplots_adjust(left=0.05, right=1.05)\n",
        "            plt.savefig(path+\"/Results/ae_lle_\"+myfile[:-4]+\".png\", format=\"png\")\n",
        "            plt.savefig(path+\"/Results/ae_lle_\"+myfile[:-4]+\".svg\", format=\"svg\")\n",
        "\n",
        "    res_file = open(path+\"/Results/ae_lle_\"+myfile+\"_res.txt\", \"w\")\n",
        "    towrite = str(mean(lle_nmis))+\",\"+str(stdev(lle_nmis))+\",\"\n",
        "    towrite += str(mean(ae_nmis))+\",\"+str(stdev(ae_nmis))+\",\"+str(mean(ae_lle_e_nmis))+\",\"+str(stdev(ae_lle_e_nmis))+\",\"\n",
        "    towrite += str(mean(ae_lle_l_nmis))+\",\"+str(stdev(ae_lle_l_nmis))+\"\\n\"\n",
        "    res_file.write(towrite)\n",
        "    towrite = str(mean(lle_aris))+\",\"+str(stdev(lle_aris))+\",\"\n",
        "    towrite += str(mean(ae_aris))+\",\"+str(stdev(ae_aris))+\",\"+str(mean(ae_lle_e_aris))+\",\"+str(stdev(ae_lle_e_aris))+\",\"\n",
        "    towrite += str(mean(ae_lle_l_aris))+\",\"+str(stdev(ae_lle_l_aris))\n",
        "    res_file.write(towrite)    \n",
        "    res_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkUnzzq-4bXr",
        "colab_type": "text"
      },
      "source": [
        "#### Images Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbmLMExZ4Br9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fcps_run import execute_algo, add_subplot\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "from scipy import io\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import manifold\n",
        "from statistics import mean, stdev\n",
        "from functools import partial\n",
        "\n",
        "from time import time  \n",
        "\n",
        "\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "\n",
        "ae_step = 20\n",
        "epoch = 5000 // ae_step\n",
        "save_res = True\n",
        "l_reg=1.0\n",
        "\n",
        "subplot_row = 1\n",
        "subplot_col = 8\n",
        "\n",
        "\n",
        "def run_kMeans(X, y, name, algo=1):\n",
        "    if algo == 1:\n",
        "      kmeans = KMeans(n_clusters=len(np.unique(y)), n_init=20, max_iter=300)\n",
        "      kmeans.fit(X)\n",
        "      label = kmeans.labels_\n",
        "    elif algo == 2:\n",
        "      gmm = GaussianMixture(n_components=len(np.unique(y)), n_init=20, max_iter=300)\n",
        "      label = gmm.fit_predict(X)\n",
        "    res_nmi = nmi(label, y)\n",
        "    res_ari = ari(label, y)      \n",
        "    print(name)    \n",
        "    print(res_nmi)\n",
        "    print(res_ari)\n",
        "    return res_nmi, res_ari\n",
        "\n",
        "\n",
        "#todo = [\"USPS\", \"MNIST\"]\n",
        "todo = [\"USPS\"]\n",
        "all_files = [i for i in datasets]\n",
        "\n",
        "mode = 2\n",
        "it = 10\n",
        "\n",
        "for myfile in all_files:\n",
        "    if mode >= 2 and myfile not in todo: continue\n",
        "    \n",
        "    print(\"#####################\")\n",
        "    print(\"#\",myfile)\n",
        "    print(\"#####################\")\n",
        "    X, y, _, _ = datasets[myfile]\n",
        "    \n",
        "    gmm_nmis, gmm_aris, lle_nmis, lle_aris, ae_nmis, ae_aris, ae_lle_e_nmis, ae_lle_e_aris, ae_lle_l_nmis, ae_lle_l_aris = [], [], [], [], [], [], [], [], [], []\n",
        "    best_mean_nmi, best_mean_ari = 0, 0\n",
        "    for i in range(it):\n",
        "        print(\"_________________________________________________________________\")\n",
        "        print(i, \" \\ \", it)\n",
        "        temp = open(path+\"/Results2/temp.txt\", \"w\")\n",
        "        temp.write(str(i))\n",
        "        temp.close()\n",
        "\n",
        "        subplot_cpt = 1\n",
        "      \n",
        "        fig = plt.figure(figsize=(15, 5))    \n",
        "\n",
        "        # Parameters\n",
        "        big_img = X.shape[1] >= 18 and X.shape[2] >= 18\n",
        "        divisor = 4 if big_img else 2\n",
        "        encoding_height, encoding_width = X.shape[1]//divisor, X.shape[2]//divisor\n",
        "        encoding_dim = encoding_height * encoding_width\n",
        "        print(\"encoding_dim = \", encoding_dim)             \n",
        "\n",
        "\n",
        "        #####################    \n",
        "        # AE LLE\n",
        "        #####################\n",
        "        print(\"AE LLE...\")        \n",
        "        t0 = time()        \n",
        "        # Create Autoencoder\n",
        "        autoencoder, encoder, decoded = make_conv_autoencoder_model(X.shape[1], X.shape[2])\n",
        "        # Create LLE\n",
        "        lle = LLE(neighbors_update=True, verbose=False, n_components=encoding_dim)\n",
        "        # Create AE-LLE\n",
        "        ae_lle = AE_LLE(autoencoder, encoder, lle, l_reg=l_reg)\n",
        "        # Train\n",
        "        ae_lle.fit(X, epoch=epoch, ae_step=ae_step)        \n",
        "        # Get embeddings\n",
        "        X_ae_lle_X_encoded = ae_lle.get_X_encoded(X)\n",
        "        X_ae_lle_Y_lle, _ = ae_lle.get_Y_lle()\n",
        "        t1 = time()\n",
        "        print(\"AE LLE : %.2g sec\" % (t1 - t0))   \n",
        "\n",
        "\n",
        "        #####################    \n",
        "        # Conv Autoencoder\n",
        "        #####################         \n",
        "        print(\"AE...\")    \n",
        "        t0 = time()\n",
        "        autoencoder, encoder, decoded = make_conv_autoencoder_model(X.shape[1], X.shape[2])\n",
        "        autoencoder.fit(X, X,\n",
        "                epochs=200,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True, verbose=0)\n",
        "        X_ae = encoder.predict(X)\n",
        "        t1 = time()\n",
        "        print(\"Conv Autoencoder : %.2g sec\" % (t1 - t0))        \n",
        "\n",
        "\n",
        "        #####################    \n",
        "        # Locally Linear Embedding\n",
        "        #####################    \n",
        "        X_lle = execute_algo(partial(manifold.LocallyLinearEmbedding, n_components=encoding_dim), X.reshape((X.shape[0], X.shape[1]*X.shape[2])), \"LLE\", subplot_cpt, y, fig)\n",
        "        subplot_cpt += 2                  \n",
        "\n",
        "\n",
        "        #####################    \n",
        "        # k-Means\n",
        "        #####################        \n",
        "        # kMeans on LLE\n",
        "        res_nmi, res_ari = run_kMeans(X_lle, y.ravel(), \"LLE\")\n",
        "        lle_nmis.append(res_nmi)\n",
        "        lle_aris.append(res_ari)        \n",
        "        \n",
        "        # kMeans on AE\n",
        "        res_nmi, res_ari = run_kMeans(X_ae, y.ravel(), \"AE\")\n",
        "        ae_nmis.append(res_nmi)\n",
        "        ae_aris.append(res_ari)       \n",
        "        \n",
        "        # kMeans on AE LLE\n",
        "        res_nmi, res_ari = run_kMeans(X_ae_lle_X_encoded, y.ravel(), \"AE LLE (X encoded)\")\n",
        "        ae_lle_e_nmis.append(res_nmi)\n",
        "        ae_lle_e_aris.append(res_ari)   \n",
        "\n",
        "        # kMeans on AE LLE\n",
        "        res_nmi, res_ari = run_kMeans(X_ae_lle_Y_lle, y.ravel(), \"AE LLE (Y lle)\")\n",
        "        ae_lle_l_nmis.append(res_nmi)\n",
        "        ae_lle_l_aris.append(res_ari)    \n",
        "\n",
        "        # Save results\n",
        "        if save_res:\n",
        "            str_l_reg = \"\" if l_reg==1.0 else \"l\"+str(l_reg)+\"_\"\n",
        "            str_num_conv = \"\" if num_conv==1 else \"c\"+str(num_conv)+\"_\"\n",
        "            file_name = path+\"/Results2/\"+myfile+\"_\"+str(ae_step)+\"_\"+str_l_reg+str_num_conv+\"ae_lle_res_\"\n",
        "            res_file = open(file_name+\"nmi.txt\", \"a\")\n",
        "            towrite = str(ae_nmis[-1])+\",\"+str(lle_nmis[-1])+\",\"+str(ae_lle_e_nmis[-1])+\",\"+str(ae_lle_l_nmis[-1])+\"\\n\"\n",
        "            res_file.write(towrite)\n",
        "            res_file.close()\n",
        "            res_file = open(file_name+\"ari.txt\", \"a\")\n",
        "            towrite = str(ae_aris[-1])+\",\"+str(lle_aris[-1])+\",\"+str(ae_lle_e_aris[-1])+\",\"+str(ae_lle_l_aris[-1])+\"\\n\"\n",
        "            res_file.write(towrite)\n",
        "            res_file.close()        \n",
        "\n",
        "\"\"\"\n",
        "towrite = str(mean(lle_nmis))+\",\"+str(stdev(lle_nmis))+\",\"\n",
        "towrite += str(mean(ae_nmis))+\",\"+str(stdev(ae_nmis))+\",\"+str(mean(ae_lle_e_nmis))+\",\"+str(stdev(ae_lle_e_nmis))+\",\"\n",
        "towrite += str(mean(ae_lle_l_nmis))+\",\"+str(stdev(ae_lle_l_nmis))+\"\\n\"\n",
        "res_file.write(towrite)\n",
        "towrite = str(mean(lle_aris))+\",\"+str(stdev(lle_aris))+\",\"\n",
        "towrite += str(mean(ae_aris))+\",\"+str(stdev(ae_aris))+\",\"+str(mean(ae_lle_e_aris))+\",\"+str(stdev(ae_lle_e_aris))+\",\"\n",
        "towrite += str(mean(ae_lle_l_aris))+\",\"+str(stdev(ae_lle_l_aris))\n",
        "res_file.write(towrite)    \n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}